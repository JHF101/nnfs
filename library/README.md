# Neural Networks from Scratch

Library that implements a neural network for scratch, with a number of different gradient and non-gradient optimizers.

### Optimizers
- Gradient Descent
- Gradient Descent with Momentum
- Delta-Bar-Delta
- Resilient Propagation (RProp)
- RMSProp
- Adaptive Moment Estimation
- Genetic Optimizer

### Initializers
- Heuristic (User specified upper and lower ranges)
- Xavier
- He
- None (Randomized between -0.5 and 0.5)

### Activation functions
- sigmoid
- tanh
- relu
- softmax
- linear

### Error/Loss functions
- mse
- squared error

<!-- ## Installation  -->

<!-- ## Usage -->
## Usage

## Fine Tuning a model
Model fine tuning can be performed ...


# References
https://ml-cheatsheet.readthedocs.io/
https://machinelearningmastery.com/cross-entropy-for-machine-learning/
https://www.v7labs.com/blog/neural-networks-activation-functions
https://mlfromscratch.com/activation-functions-explained/#/
https://deepnotes.io/softmax-crossentropy

https://towardsdatascience.com/documenting-python-code-with-sphinx-554e1d6c4f6d